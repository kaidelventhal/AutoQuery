README.md:
<code>
# AutoQuery AI

**Live Demo:** [Access the AutoQuery AI Assistant](https://autoquery-454320.uc.r.appspot.com/)

## Project Description

AutoQuery AI is an intelligent vehicle recommendation and query assistant deployed on Google Cloud. It helps users find vehicle information by asking questions in natural language. Leveraging Google Vertex AI's language models and LangChain for orchestration, AutoQuery AI translates user requests Agenticaly into SQL queries executed against a vehicle dataset stored in Google Cloud Storage.

## How It Works

1.  **Frontend Interaction:** Users interact with a simple web interface (HTML/CSS/JavaScript) served by Google App Engine.
2.  **API Request:** User messages are sent to a backend API built with Flask and deployed as a separate App Engine service.
3.  **Agent Processing:** The Flask backend uses a LangChain `AgentExecutor`. This agent is powered by a Google Vertex AI LLM (currently configured with Gemini Flash Lite).
4.  **Prompting & Schema:** The agent receives the user query along with a detailed system prompt containing:
    * Instructions on how to behave.
    * The exact database schema (table names, column names, data types).
    * Rules for generating SQL (case-insensitivity, handling specific column names like `Maker` vs `Automaker`, single-statement execution).
    * Guidance on validating user input against known data (e.g., checking model names).
5.  **SQL Generation:** Based on the user query and its prompt, the LLM generates a single `pandasql` (SQLite syntax) query.
6.  **Tool Execution:** The agent invokes a custom LangChain tool (`execute_sql`).
7.  **Data Querying:**
    * The `execute_sql` tool uses the `pandasql` library.
    * `pandasql` runs the generated SQL query against Pandas DataFrames.
    * These DataFrames are loaded into memory by the backend application *on startup* from CSV files stored in a Google Cloud Storage (GCS) bucket.
8.  **Result Formatting:** The tool returns the query results as a CSV-formatted string (or an error message) back to the agent.
9.  **Response Generation:** The agent analyzes the tool's output (the CSV data or error) and formulates a final, user-friendly natural language response.
10. **API Response:** The Flask backend sends the agent's response back to the frontend, which displays it to the user.

## Example Prompts to Try

Here are a few example questions you can ask to test the agent's capabilities:

* **Simple Lookup & Sorting:**
    * `What was the top selling car in 2015?`
    * `What car had the largest engine size?`
    * `Which car has the highest top speed?`
* **Filtering:**
    * `Show me red Ford Fiesta cars registered after 2018` (Tests case-insensitivity)
    * `List BMW cars registered in 2020 with less than 10000 miles`
    * `Find cars with an automatic gearbox and more than 200 engine power`
* **Joins / Combining Info (Implicit):**
    * `What was the entry price for a Ford Focus in 2019?` (Uses `price_table`)
    * `Show sales data for the Ford F150 in 2016` (Tests model name handling "F150")
* **Testing Limitations:**
    * `List the top selling cars for 2018 and 2019` (Agent should state it can only do one year at a time)
    * `What data do you have for the 'CyberTruck' model?` (Agent should report no data found if it's not in the tables)

Feel free to experiment with different combinations of makes, models, years, colors, features, etc.!

## Tech Stack

* **Cloud Platform:** Google Cloud
    * **Compute:** App Engine Standard (Python 3.12 Runtime)
    * **Storage:** Cloud Storage (for CSV data)
    * **AI:** Vertex AI (Gemini Flash Lite Model)
* **Backend:**
    * **Language:** Python
    * **Framework:** Flask
    * **Web Server:** Gunicorn (with `gthread` workers)
    * **API Communication:** Flask-CORS
* **AI Orchestration:** LangChain (AgentExecutor, Tools, Prompts)
* **Data Querying:** Pandas, Pandasql
* **Frontend:** HTML, CSS, Vanilla JavaScript

## Key Features

* **Natural Language Interface**: Ask questions in plain English to query vehicle data.
* **Agentic SQL Generation**: Converts natural language to `pandasql` queries using an LLM agent.
* **Cloud-Native Deployment**: Runs efficiently on Google App Engine, utilizing Cloud Storage for data.
* **Error Handling**: Agent attempts to identify and report SQL execution errors.

## Getting Started (for Contributors)

While the primary way to use the app is via the live demo link above, contributors wishing to run or modify the code locally can follow these steps:

### Prerequisites

* Python 3.10+
* Google Cloud SDK (`gcloud`) installed and authenticated (`gcloud auth login`, `gcloud auth application-default login`)
* A Google Cloud Project with Billing enabled.
* APIs Enabled: App Engine Admin, Vertex AI, Cloud Storage, Cloud Build.
* A GCS Bucket containing the required CSV data files (`Ad_table.csv`, `Price_table.csv`, etc.) in a specific folder structure (e.g., `tables_V2.0/`).

### Installation

1.  Clone the repository:
    ```bash
    git clone [https://github.com/yourusername/autoquery-ai.git](https://github.com/yourusername/autoquery-ai.git) # Replace with your repo URL
    cd autoquery-ai
    ```
2.  Set up a Python virtual environment (recommended):
    ```bash
    python -m venv venv
    source venv/bin/activate # Linux/macOS
    # venv\Scripts\activate # Windows
    ```
3.  Install required packages:
    ```bash
    pip install -r backend/requirements.txt
    ```
4.  **Configure Environment Variables (Crucial for Local Backend):**
    Set the following environment variables in your local terminal session *before* running the backend:
    ```bash
    export GOOGLE_CLOUD_PROJECT="your-gcp-project-id"
    export TABLES_BUCKET="your-gcs-bucket-name"
    export TABLES_FOLDER="your-folder-in-bucket" # e.g., tables_V2.0 or "" if root
    # Use 'set' instead of 'export' on Windows Command Prompt
    # Use '$env:VAR_NAME = "value"' in PowerShell
    ```
    *(Ensure your local machine is authenticated via `gcloud auth application-default login` for the backend to access GCS and Vertex AI).*

### Local Usage

1.  **Run Backend:**
    ```bash
    # Navigate to the backend directory
    cd backend
    # Run using Flask's development server (for testing, less robust than gunicorn)
    flask run --host=0.0.0.0 --port=5000
    # OR run with gunicorn locally (mimics App Engine better)
    # gunicorn -b 0.0.0.0:5000 -w 1 -k gthread app:app --log-level debug
    ```
2.  **Run Frontend:**
    * Open `frontend/index.html` directly in your browser (will likely fail due to CORS if backend isn't on `localhost:5000`).
    * OR use a simple local web server (like Python's `http.server` or VS Code Live Server) from the `frontend` directory. You may need to adjust the `API_URL` in `script.js` temporarily to `http://localhost:5000/api/chat` for local testing. Remember to change it back before deploying the frontend.

## Deployment

The application is designed for Google App Engine Standard Environment using two services:

1.  **`backend` Service:** Runs the Python/Flask application defined in `backend/app.yaml`. Deployed via `gcloud app deploy backend/app.yaml`.
2.  **`default` Service (or `frontend`):** Serves the static frontend files (HTML/CSS/JS) defined in `frontend/app.yaml`. Deployed via `gcloud app deploy frontend/app.yaml`.

*(Ensure `app.yaml` files have correct project IDs, environment variables, and instance classes before deploying.)*

## Future Enhancements

* **React Frontend:** Migrate the frontend from Vanilla JS to React for a more modern, component-based UI, better state management, and easier development of complex features.
* **Improved Agentic Ability:**
    * **Multi-Step Reasoning:** Implement frameworks like LangGraph to allow the agent to perform multiple sequential `execute_sql` calls for complex requests (e.g., comparing data across years, complex validation steps).
    * **Error Recovery:** Enhance the agent's ability to automatically correct and retry failed SQL queries based on error messages.
    * **Data Visualization:** Add capabilities for the agent to generate simple charts or summaries based on query results.
    * **Image Integration:** Utilize the `img_table` to display relevant vehicle images alongside query results.
* **Enhanced Security:**
    * **Authentication:** Add user login/authentication (e.g., using Google Identity Platform or Firebase Auth) if multi-user support or data protection is needed.
    * **Input Sanitization:** Implement stricter validation and sanitization on user inputs and potentially LLM outputs.
    * **Secrets Management:** Use Google Secret Manager for any API keys or sensitive configuration if needed (currently relies on ADC).
    * **Stricter CORS:** Configure `Flask-CORS` to only allow requests specifically from the deployed frontend origin instead of `*`.
    * **Rate Limiting:** Implement API rate limiting (e.g., using `Flask-Limiter`) to prevent abuse.
* **Data Backend Migration:** For improved performance, scalability, and reduced memory usage (allowing smaller App Engine instances), migrate the data from GCS CSV files + Pandas DataFrames to a dedicated database like Google Cloud SQL (PostgreSQL/MySQL) or potentially BigQuery. This would require replacing `pandasql` with standard SQL querying libraries (like SQLAlchemy or database-specific connectors).

## Conclusion

AutoQuery AI demonstrates the power of combining LLMs (Vertex AI), orchestration frameworks (LangChain), cloud services (GCP App Engine, GCS), and data manipulation libraries (Pandas, Pandasql) to create interactive, data-driven applications. It showcases skills in backend development, API design, cloud deployment, and AI integration.
</code>

backend\agents.py:
<code>
# agents.py
from langchain_google_vertexai import ChatVertexAI
from langchain.agents import AgentExecutor
from langchain.agents.format_scratchpad.openai_tools import format_to_openai_tool_messages
from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser
from prompts import get_sql_generation_prompt
from langchain.schema import AIMessage, HumanMessage
from agent_tools import execute_sql

# Define the list of tools available to the agent.
tools = [execute_sql]

def create_sql_agent():
    """
    Create and return an agent executor that uses the Gemini model with tool support.
    """
    llm = ChatVertexAI(
        model="gemini-2.0-flash-lite",
        temperature=0,
        convert_system_message_to_human=True,
        project = "autoquery-454320"
    )
    llm_with_tools = llm.bind_tools(tools)
    
    prompt = get_sql_generation_prompt()
    
    def input_extractor(x):
        return x["input"]
    
    def scratchpad_formatter(x):
        return format_to_openai_tool_messages(x["intermediate_steps"])
    
    agent_components = {
        "input": input_extractor,
        "agent_scratchpad": scratchpad_formatter,
        "chat_history": lambda x: x["chat_history"]
    }
    
    agent = agent_components | prompt | llm_with_tools | OpenAIToolsAgentOutputParser()
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    return agent_executor

</code>

backend\agent_tools.py:
<code>
# agent_tools.py
from pydantic import BaseModel, Field
from langchain.agents import tool
from database import Database # Import the NEW Database class

# This global variable will hold the database instance set by app.py
db_instance: Database | None = None # Added type hint for clarity

def set_database_instance(db: Database):
    """Sets the shared database instance."""
    global db_instance
    print(f"Setting database instance in agent_tools: {type(db)}") # Add print for debugging
    db_instance = db

class SQLQueryInput(BaseModel):
    query: str = Field(..., description="The SQL query to execute on the automotive database.")

@tool
def execute_sql(query: str) -> str:
    """
    Execute the provided SQL query on the automotive database (Cloud SQL) and return the results as CSV.
    Use this tool to run SELECT queries against the available tables (ad_table, price_table, sales_table, etc.).
    Provide the complete standard SQL query as the input string.
    """
    print(f"Executing SQL via tool. DB instance type: {type(db_instance)}") # Add print for debugging
    if db_instance is None:
        print("Error: execute_sql called but database instance is None.") # Add print for debugging
        return "Error: Database not initialized in the application."
    try:
        # db_instance is now an instance of the new Database class connecting to Cloud SQL
        result_csv = db_instance.run_query(query)
        # Ensure the result is a string. run_query should return CSV string or error string.
        if isinstance(result_csv, str):
            # Limit result size to prevent excessively large responses if necessary
            max_length = 5000 # Example limit, adjust as needed
            if len(result_csv) > max_length:
                # Truncate large results
                truncated_msg = "\n... (results truncated)"
                # Try to keep header if possible
                header_end = result_csv.find('\n')
                if header_end != -1:
                    header = result_csv[:header_end+1]
                    return header + result_csv[header_end+1:max_length - len(truncated_msg) - len(header)] + truncated_msg
                else: # If no newline (single line result?)
                     return result_csv[:max_length - len(truncated_msg)] + truncated_msg
            return result_csv
        else:
            # Should not happen if run_query works as expected, but handle just in case
            print(f"Warning: run_query returned unexpected type: {type(result_csv)}")
            return "Error: Query execution returned an unexpected data type."
    except Exception as e:
        print(f"Error during execute_sql tool execution: {e}", exc_info=True) # Log exception
        # Return a user-friendly error message
        return f"Error executing SQL query via tool: {str(e)}"
</code>

backend\app.py:
<code>
# backend/app.py

from flask import Flask, request, jsonify
from flask_cors import CORS
from agents import create_sql_agent
from agent_tools import set_database_instance
from database import Database # Import the SQLite Database class
from langchain.schema import AIMessage, HumanMessage
import os
import logging
import signal
import sys

logging.basicConfig(level=logging.INFO)

# --- Initialize Database ---
db = None
try:
    # Instantiate the SQLite Database class
    db = Database()
    set_database_instance(db)
    # Optional: Add a check here to see if the DB file actually exists
    # This relies on the logic within Database.__init__ and run_query
    logging.info("Database object initialized for SQLite file.")
except Exception as e:
    logging.error(f"FATAL: Failed to initialize Database object: {e}", exc_info=True)
    db = None
    set_database_instance(None)

# --- Initialize Agent ---
agent_executor = None
if db: # Check if db object was created (doesn't guarantee file exists yet)
    try:
        agent_executor = create_sql_agent()
        logging.info("LangChain Agent created successfully.")
    except Exception as e:
        logging.error(f"FATAL: Failed to create LangChain agent: {e}", exc_info=True)
        agent_executor = None
else:
    logging.warning("Database object initialization failed, skipping agent creation.")

chat_history = []
app = Flask(__name__)
# This CORS config SHOULD work once the 500 error is fixed
CORS(app, resources={r"/api/*": {"origins": "*"}})

@app.route('/api/chat', methods=['POST', 'OPTIONS'])
def chat():
    # If it is an OPTIONS request, return an empty successful response
    if request.method == 'OPTIONS':
        # Return an empty 200 response with the required headers
        response = app.make_default_options_response()
        # Optionally, you can add additional CORS headers here if needed:
        headers = response.headers
        headers['Access-Control-Allow-Origin'] = '*'
        headers['Access-Control-Allow-Methods'] = 'POST, OPTIONS'
        headers['Access-Control-Allow-Headers'] = 'Content-Type'
        return response

    # For POST, continue with processing
    data = request.get_json()
    if not data or "message" not in data:
        logging.warning("Received invalid chat request: Missing 'message'")
        return jsonify({"error": "Missing 'message' in request body"}), 400

    # ... rest of your POST processing
    result = agent_executor.invoke({"input": data.get("message"), "chat_history": chat_history})
    response_content = result.get("output", "Agent did not return an output.")
    chat_history.append(HumanMessage(content=data.get("message")))
    chat_history.append(AIMessage(content=response_content))
    return jsonify({"response": response_content})

@app.route('/_ah/warmup')
def warmup():
    logging.info("Warmup request received.")
    # --- CORRECTED CHECK ---
    if db is None:
         logging.warning("Warmup: Database object not initialized.")
    # You could try calling db._get_connection() here to test connectivity early
    # try:
    #      conn = db._get_connection()
    #      conn.close()
    #      logging.info("Warmup: DB connection test successful.")
    # except Exception as e:
    #      logging.error(f"Warmup: DB connection test failed: {e}")
    # --- End Corrected Check ---
    return '', 200, {}

@app.route('/healthz')
def healthz():
    # --- CORRECTED CHECK ---
     if db: # Basic check: is the Database object instantiated?
         # More robust: try a quick connection/query
         try:
              conn = db._get_connection()
              # Optional: Run a super simple query like "SELECT 1"
              # cursor = conn.cursor()
              # cursor.execute("SELECT 1")
              # cursor.fetchone()
              conn.close()
              return "OK", 200
         except Exception as e:
              logging.error(f"Health check DB connection failed: {e}")
              return "Service Unavailable (DB connection failed)", 503
     else:
          return "Service Unavailable (DB init failed)", 503
    # --- End Corrected Check ---

# --- Graceful Shutdown Handling ---
# Remove db.close_connection() call as SQLite connections are short-lived
def cleanup(signum, frame):
    logging.info("Received shutdown signal. Cleaning up...")
    # No persistent DB connection pool to close for SQLite used this way
    logging.info("Cleanup complete (no persistent DB pool to close). Exiting.")
    sys.exit(0)

signal.signal(signal.SIGTERM, cleanup)
signal.signal(signal.SIGINT, cleanup)

# --- Main Execution ---
if __name__ == '__main__':
    logging.warning("Running Flask development server (for local testing only).")
    # Ensure DB file exists locally for testing
    if db:
         logging.info(f"Local testing with DB path: {db.db_path}")
    else:
         logging.error("DB object not created for local testing.")
    app.run(debug=False, port=int(os.environ.get('PORT', 8080)), host='0.0.0.0')
</code>

backend\app.yaml:
<code>
runtime: python312
service: backend # Or your desired service name
entrypoint: gunicorn -b :$PORT -w 2 --threads 4 -k gthread app:app --timeout 120 --log-level info

instance_class: F4 # Use F1 for free tier eligibility, monitor performance/memory

env_variables:
  GOOGLE_CLOUD_PROJECT: "autoquery-454320" # Needed for Vertex AI client
  # Add any other non-secret environment variables your app needs
  # NO database connection vars needed here

automatic_scaling:
  min_instances: 0 # Allows scaling to zero for cost savings
  max_instances: 2 # Set a reasonable limit for a personal project
  target_cpu_utilization: 0.75
  # Consider target_concurrent_requests if needed

# Ensure all necessary APIs are enabled in your GCP project (App Engine, Vertex AI)

handlers:
- url: /.* # Route all traffic to your app
  script: auto
  secure: always # Enforce HTTPS

# Add warmup handler if needed (app.py includes a basic one)
inbound_services:
  - warmup
</code>

backend\database.py:
<code>
# backend/database.py
import os
import sqlite3
import pandas as pd
import io
import logging

logging.basicConfig(level=logging.INFO)

# --- Configuration ---
# Determine the absolute path to the directory containing this script
_BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# Construct the path to the database file relative to this script's location
DB_FILENAME = "autoquery_data.db"
DB_FILE_PATH = os.path.join(_BASE_DIR, DB_FILENAME)
# --- End Configuration ---

class Database:
    def __init__(self):
        self.db_path = DB_FILE_PATH
        logging.info(f"Database object initialized for SQLite file: {self.db_path}")
        if not os.path.exists(self.db_path):
             # This might happen during local testing if the DB isn't generated yet,
             # but should not happen in App Engine if deployment included the file.
             logging.warning(f"Database file not found at {self.db_path} during init.")

    def _get_connection(self):
        """Establishes a read-only connection to the SQLite database."""
        try:
            if not os.path.exists(self.db_path):
                 raise FileNotFoundError(f"SQLite DB file not found: {self.db_path}")
            # Connect using file path and READ-ONLY mode
            # uri=True allows using mode=ro parameter
            conn = sqlite3.connect(f"file:{self.db_path}?mode=ro", uri=True, timeout=10)
            logging.info(f"Connected to SQLite DB (read-only): {self.db_path}")
            return conn
        except sqlite3.Error as e:
            # Check if error is because file doesn't exist or isn't a DB
            logging.error(f"Error connecting to SQLite database {self.db_path}: {e}", exc_info=True)
            raise

    def run_query(self, query: str) -> str:
        """Executes a SQL query against the read-only SQLite database."""
        logging.info(f"Attempting to execute SQLite query (RO mode): {query[:500]}...")
        conn = None
        try:
            conn = self._get_connection()
            # Using pandas read_sql_query still works fine with the connection object
            result_df = pd.read_sql_query(query, conn)
            logging.info(f"Query returned {len(result_df)} rows.")

            if result_df.empty:
                # Return headers only for empty results
                return ",".join(result_df.columns) + "\n" if result_df.columns.tolist() else ""
            else:
                csv_buffer = io.StringIO()
                result_df.to_csv(csv_buffer, index=False)
                csv_output = csv_buffer.getvalue()
                return csv_output

        except FileNotFoundError as e:
             logging.error(f"Query failed because DB file was not found: {e}")
             return f"Error: Database file missing at {self.db_path}."
        except sqlite3.OperationalError as db_err:
             # Specific check for write attempts on read-only DB
             if "attempt to write a readonly database" in str(db_err):
                 logging.error(f"Write rejected on read-only DB. Query: '{query[:200]}...'", exc_info=False)
                 return f"Error: Database is read-only ({db_err})"
             else:
                 logging.error(f"SQLite Operational Error: {db_err}. Query: '{query[:200]}...'", exc_info=True)
                 return f"SQL Execution Error: {str(db_err)}"
        except sqlite3.Error as db_err:
            logging.error(f"General SQLite Error: {db_err}. Query: '{query[:200]}...'", exc_info=True)
            return f"SQL Execution Error: {str(db_err)}"
        except Exception as e:
            logging.error(f"Unexpected error during query execution: {e}", exc_info=True)
            return f"Unexpected Error: {str(e)}"
        finally:
            if conn:
                conn.close()
                logging.info("SQLite connection closed.")
</code>

backend\main.py:
<code>
# main.py
import os
import logging
from database import Database # Import the NEW Database class
from agent_tools import set_database_instance
from agents import create_sql_agent
from langchain.schema import AIMessage, HumanMessage

# --- FOR LOCAL TESTING ONLY ---
# Requires setting environment variables for DB connection:
# GOOGLE_CLOUD_PROJECT, INSTANCE_CONNECTION_NAME, DB_USER, DB_PASS, DB_NAME
# Recommend running the Cloud SQL Auth Proxy locally for secure connection.

logging.basicConfig(level=logging.INFO)

def main():
    print("--- Running Local Test Mode (Connecting to Cloud SQL) ---")
    print("Ensure DB environment variables are set and Cloud SQL Auth Proxy is running (recommended).")

    db = None
    agent_executor = None

    # --- Initialize Database Connection ---
    try:
        db = Database() # Reads env vars internally
        set_database_instance(db)
        print("Database connection pool initialized successfully.")
    except Exception as e:
        print(f"FATAL: Failed to initialize database connection: {e}")
        print("Please check environment variables and Cloud SQL proxy/network.")
        return # Exit if DB connection fails

    # --- Initialize Agent ---
    try:
        agent_executor = create_sql_agent()
        print("LangChain Agent created successfully.")
    except Exception as e:
        print(f"FATAL: Failed to create LangChain agent: {e}")
        return # Exit if agent creation fails

    # In-memory chat history (list of LangChain message objects).
    chat_history = []

    print("\nWelcome to the AutoSQL Chat Interface (Local Cloud SQL Test Mode)!")
    print("Enter your natural language queries (type 'exit' to quit).")

    while True:
        user_input = input("You: ").strip()
        if user_input.lower() == "exit":
            break

        if db is None or db.pool is None or agent_executor is None:
            print("Agent: Cannot process query. Database or Agent not ready.")
            continue

        agent_input = {
            "input": user_input,
            "chat_history": chat_history,
        }

        try:
            result = agent_executor.invoke(agent_input)
            agent_output = result.get("output", "No output returned.")

            print("Agent:", agent_output)

            # Update the in-memory chat history.
            chat_history.append(HumanMessage(content=user_input))
            chat_history.append(AIMessage(content=agent_output))

            # Optional: Limit history size locally too
            max_history = 20
            if len(chat_history) > max_history:
                chat_history = chat_history[-max_history:]

        except Exception as e:
            print(f"\nError during local agent invocation: {e}")
            logging.error("Local agent invocation error:", exc_info=True) # Log stack trace


    # Cleanup connections on exit
    if db:
        db.close_connection()
    print("Exiting local test mode.")

if __name__ == "__main__":
    main()
</code>

backend\prompts.py:
<code>
# backend/prompts.py
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

def get_sql_generation_prompt():
    """
    Create a prompt template that instructs the agent to convert a natural language query
    into a valid SQL query for the automotive database hosted on Cloud SQL (PostgreSQL).
    """
    # --- Updated System Message for Standard SQL (PostgreSQL) ---
    system_message = """
        You are an AI assistant specialized in querying an automotive database hosted on Google Cloud SQL (using standard PostgreSQL syntax).
        Your goal is to answer user questions accurately by generating AND executing SQL queries against the available tables.

        **CRITICAL RULE:** You MUST generate only **ONE single valid SQL statement** per request to the `execute_sql` tool. Do NOT include multiple statements separated by semicolons or newlines in the `query` parameter for the tool. If a user asks a question that requires multiple independent queries (e.g., 'top seller for each year'), you MUST inform the user that you can only process one part at a time (e.g., 'I can find the top seller for a specific year. Please specify which year you're interested in.').
        **1. Table Schemas and Relationships (SQLite Database File):**
        The database is a single SQLite file (`autoquery_data.db`) included with the application, containing the following tables:

        * `ad_table`: Columns: `Maker`, `Genmodel`, `Genmodel_ID`, `Adv_ID`, `Adv_year`, `Adv_month`, `Color`, `Reg_year`, `Bodytype`, `Runned_Miles`, `Engin_size`, `Gearbox`, `Fuel_type`, `Price`, `Engine_power`, `Annual_Tax`, `Wheelbase`, `Height`, `Width`, `Length`, `Average_mpg`, `Top_speed`, `Seat_num`, `Door_num`. (Standard SQLite types like TEXT, INTEGER, REAL).
        * `price_table`: Columns: `Maker`, `Genmodel`, `Genmodel_ID`, `Year`, `Entry_price`.
        * `sales_table`: Columns: `Maker`, `Genmodel`, `Genmodel_ID`, `"2020"`, `"2019"`, `"2018"`, ..., `"2001"`. (IMPORTANT: Year columns are likely TEXT and named like `"2018"`. They MUST be double-quoted in the SQL query, e.g., `SELECT "2018" FROM sales_table`.)
        * `basic_table`: Columns: `Automaker`, `Automaker_ID`, `Genmodel`, `Genmodel_ID`. **Warning:** Uses `Automaker`, others use `Maker`.
        * `trim_table`: Columns: `Genmodel_ID`, `Maker`, `Genmodel`, `Trim`, `Year`, `Price`, `Gas_emission`, `Fuel_type`, `Engine_size`.
        * `img_table`: Columns: `Genmodel_ID`, `Image_ID`, `Image_name`, `Predicted_viewpoint`, `Quality_check`.

        **IMPORTANT:** The database is read-only. Do not generate UPDATE, INSERT, or DELETE statements.

        **2. Common Join Keys:**
        * Primary join fields: `Genmodel_ID`. Also `Maker`/`Automaker` and `Genmodel` where available. Use standard SQL JOIN syntax (`INNER JOIN`, `LEFT JOIN`, etc.).

        **3. Query Best Practices (SQLite):**
        * **Single Statement ONLY:** Generate only one SELECT statement.
        * **Quoted Identifiers:** Only quote table or column names if they contain special characters or spaces (like the year columns in `sales_table`, e.g., `"2018"`). Standard identifiers (`Maker`, `ad_table`) usually don't need quotes.
        * **Case-Insensitive Filtering:** For string comparisons (`Maker`, `Genmodel`, `Color`), use the `UPPER()` or `LOWER()` function on both the column and the literal value. Example: `WHERE UPPER(Maker) = UPPER('Ford')`. SQLite's `LIKE` is case-insensitive by default for ASCII characters, but using `UPPER`/`LOWER` is safer and clearer.
        * **Table Aliases:** Use aliases in JOINs (e.g., `FROM ad_table AS ad JOIN basic_table AS b ON ad.Genmodel_ID = b.Genmodel_ID`).
        * **Valid Columns:** Ensure selected columns exist. Pay attention to `Maker` vs. `Automaker`.
        * **NULL Handling:** Use `IS NULL` or `IS NOT NULL`.
        * **Data Types:** Use standard SQL type casting if necessary (e.g., `CAST(some_column AS INTEGER)`).

        **4. Handling Potential Data Issues:**
        * **Model Name Variations:** User input for model names (`Genmodel`) might differ slightly (e.g., 'F-150' vs 'F150'). If a query returns no results for a specific model, consider verifying the canonical name in `basic_table` using `SELECT DISTINCT Genmodel FROM basic_table WHERE Automaker ILIKE '...'`. Inform the user or ask for clarification if needed. Do *not* run this verification query unless the primary query fails with zero results for a specific model filter.

        **5. Execution Workflow:**
        1. Analyze the user's question.
        2. Identify the necessary table(s) and column(s).
        3. Generate **ONE single, valid standard SQL query** following best practices.
        4. Use the `execute_sql` tool, passing the generated SQL query string.
        5. Analyze the results (CSV data or error message) from the tool.
        6. Formulate a natural language answer based *only* on the tool's output. Summarize findings or present data clearly. If the tool returns an error (e.g., "SQL Execution Error..."), report it clearly and explain the likely cause. If the tool returns an empty result (just CSV headers), state that no matching data was found.

        **Your final output MUST be the natural language answer derived from the executed query results or a clear explanation of why the query failed/returned no data. Do NOT output the SQL query itself unless specifically asked.**
        """

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_message),
        MessagesPlaceholder(variable_name="chat_history"),
        ("user", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad")
    ])
    return prompt
</code>

frontend\app.yaml:
<code>
# frontend/app.yaml

# Specify a valid runtime, even though we only serve static files.
# This satisfies the App Engine requirement.
runtime: python312 # Or choose another valid runtime like nodejs20, etc.

service: default # Deploy as the default service

handlers:
  # Serve the CSS file
  - url: /style\.css
    static_files: style.css # Path relative to this app.yaml file
    upload: style\.css      # Regex matching the file to upload

  # Serve the JavaScript file
  - url: /script\.js
    static_files: script.js
    upload: script\.js

  # Serve the main index.html for the root path and any other unmatched paths
  - url: /.*
    static_files: index.html
    upload: index\.html

# Optional: Redirect all traffic to HTTPS (Good practice)
# secure: always

# Optional: Set expiration for static assets
# default_expiration: "1d"

# Optional: Specify scaling if needed, otherwise defaults apply
# automatic_scaling:
#  min_instances: 0
#  max_instances: 1 # Often 1 is sufficient for purely static frontend
</code>

frontend\index.html:
<code>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AutoQuery AI</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>AutoQuery AI</h1>
            <h2>Vehicle Data Query Assistant built by Kai Delventhal</h2>
        </header>

        <div class="chat-window">
            <ul id="message-list">
                <!-- Messages will be added here by JavaScript -->
            </ul>
        </div>

        <div id="loading-indicator" class="loading" style="display: none;">
            <span>Agent thinking...</span>
        </div>

         <div id="error-display" class="error-message" style="display: none;">
             <!-- Errors will be shown here -->
         </div>

        <form id="chat-form" class="chat-input-area">
            <input type="text" id="message-input" placeholder="Ask about vehicle data..." required autocomplete="off">
            <button type="submit" id="send-button">Send</button>
        </form>

        <footer>
            <p>Powered by Google Vertex AI & LangChain</p>
        </footer>
    </div>

    <script src="script.js"></script>
</body>
</html>
</code>

frontend\script.js:
<code>
document.addEventListener('DOMContentLoaded', () => {
    const messageList = document.getElementById('message-list');
    const chatForm = document.getElementById('chat-form');
    const messageInput = document.getElementById('message-input');
    const sendButton = document.getElementById('send-button');
    const loadingIndicator = document.getElementById('loading-indicator');
    const errorDisplay = document.getElementById('error-display');

    // --- Configuration ---
    // IMPORTANT: After deploying the backend service, update this URL!
    // Get the backend service URL from `gcloud app deploy backend/app.yaml` output
    // (e.g., https://backend-dot-your-project-id.appspot.com)
    // and append '/api/chat' to it.
    const API_URL = 'https://backend-dot-autoquery-454320.uc.r.appspot.com/api/chat'; // <<< UPDATE THIS AFTER BACKEND DEPLOYMENT
    // Example Deployed URL: const API_URL = 'https://backend-dot-your-project-id.appspot.com/api/chat';

    // In-memory chat history (client-side only for display)
    // Note: The actual history context for the LLM is managed server-side in app.py
    let chatHistory = [
        { sender: 'agent', message: 'Welcome to AutoQuery AI! How can I help you find vehicle data today?' }
    ];

    // --- Functions ---

    /** Renders messages from chatHistory to the DOM */
    function renderMessages() {
        messageList.innerHTML = ''; // Clear existing messages
        chatHistory.forEach(msg => {
            const listItem = document.createElement('li');
            listItem.classList.add('message');
            listItem.classList.add(msg.sender === 'user' ? 'user-message' : 'agent-message');

            // Sanitize output - Use textContent to prevent XSS
            listItem.textContent = msg.message;

            messageList.appendChild(listItem);
        });
        // Scroll to the bottom
        // Use setTimeout to allow the DOM to update before scrolling
        setTimeout(() => {
             messageList.scrollTop = messageList.scrollHeight;
        }, 0);
    }

    /** Displays error messages */
    function displayError(errorMessage) {
        errorDisplay.textContent = `Error: ${errorMessage}`;
        errorDisplay.style.display = 'block';
        loadingIndicator.style.display = 'none'; // Hide loading if error occurs
    }

    /** Hides the error display */
    function clearError() {
        errorDisplay.textContent = '';
        errorDisplay.style.display = 'none';
    }

    /** Sends message to backend API */
    async function sendMessageToApi(userMessage) {
        loadingIndicator.style.display = 'block';
        clearError();
        setInteractionState(true); // Disable input

        // Add user message to display history immediately
        chatHistory.push({ sender: 'user', message: userMessage });
        renderMessages();
        messageInput.value = ''; // Clear input field


        // Note: We are NOT sending the client-side history here.
        // The backend maintains its own history per instance.
        // If you needed persistent history across requests, the backend
        // would need to load/save it, and the API call might include a session ID.
        try {
            const response = await fetch(API_URL, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    message: userMessage,
                    // Not sending history from client: history: historyForApi,
                }),
            });

            if (!response.ok) {
                let errorMsg = `Request failed (${response.status})`;
                try {
                    // Try to get more specific error from backend response
                    const errorData = await response.json();
                    errorMsg = errorData.error || errorMsg;
                } catch (e) { /* Ignore if no JSON body or parsing error */ }
                throw new Error(errorMsg);
            }

            const data = await response.json();

            // Add agent response to history and re-render
            chatHistory.push({ sender: 'agent', message: data.response || "Received empty response." });
            renderMessages();

        } catch (error) {
            console.error('API Error:', error);
            displayError(error.message || 'Could not connect to the agent.');
            // Optional: Remove the user's message from display if API call failed?
            // chatHistory.pop(); // Removes the last added message (the user's)
            // renderMessages();
        } finally {
            loadingIndicator.style.display = 'none';
            setInteractionState(false); // Re-enable input
        }
    }

    /** Enables/Disables input field and send button */
    function setInteractionState(disabled) {
        messageInput.disabled = disabled;
        sendButton.disabled = disabled;
        // Optionally change styles for disabled state
        messageInput.style.cursor = disabled ? 'not-allowed' : '';
        sendButton.style.cursor = disabled ? 'not-allowed' : 'pointer';
    }


    // --- Event Listeners ---

    chatForm.addEventListener('submit', (event) => {
        event.preventDefault(); // Prevent page reload
        const userMessage = messageInput.value.trim();

        if (userMessage && !sendButton.disabled) { // Check if input is not disabled
            // Send message to backend (which also handles rendering user message)
            sendMessageToApi(userMessage);
        }
    });

    // --- Initial Render ---
    renderMessages();

}); // End DOMContentLoaded
</code>

frontend\style.css:
<code>
body {
    font-family: system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
    background-color: #1a1d21; /* Dark background */
    color: #e0e0e0; /* Light text */
    margin: 0;
    padding: 0;
    display: flex;
    justify-content: center;
    align-items: flex-start; /* Align container to top */
    min-height: 100vh;
    padding-top: 20px; /* Add some space at the top */
}

.container {
    width: 100%;
    max-width: 700px; /* Limit width */
    background-color: #282c34; /* Slightly lighter dark */
    border-radius: 8px;
    box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3);
    padding: 20px;
    display: flex;
    flex-direction: column;
}

header {
    text-align: center;
    margin-bottom: 15px;
    border-bottom: 1px solid #444;
    padding-bottom: 15px;
}

header h1 {
    margin: 0 0 5px 0;
    color: #61dafb; /* React-like blue */
}

header h2 {
    margin: 0;
    font-size: 1.1em;
    font-weight: 400;
    color: #aaa;
}

.chat-window {
    height: 60vh; /* Fixed height for chat */
    overflow-y: auto; /* Enable scrolling */
    border: 1px solid #444;
    border-radius: 5px;
    margin-bottom: 15px;
    padding: 10px;
    background-color: #1e1f22; /* Darker chat background */
    display: flex; /* Needed for scroll behavior */
    flex-direction: column; /* Stack messages */
}

#message-list {
    list-style: none;
    padding: 0;
    margin: 0;
    flex-grow: 1; /* Allow list to grow */
}

.message {
    margin-bottom: 12px;
    padding: 8px 12px;
    border-radius: 15px; /* Bubble effect */
    max-width: 80%;
    word-wrap: break-word; /* Prevent long words from overflowing */
    line-height: 1.4;
}

.user-message {
    background-color: #007bff; /* Blue for user */
    color: white;
    align-self: flex-end; /* Align user messages to right */
    border-bottom-right-radius: 4px; /* Flatten one corner */
    margin-left: auto; /* Push to right */
}

.agent-message {
    background-color: #495057; /* Gray for agent */
    color: white;
    align-self: flex-start; /* Align agent messages to left */
    border-bottom-left-radius: 4px; /* Flatten one corner */
    margin-right: auto; /* Push to left */
}

.agent-message strong, .user-message strong {
    display: block;
    font-size: 0.8em;
    margin-bottom: 4px;
    opacity: 0.8;
}

.chat-input-area {
    display: flex;
    margin-top: 10px; /* Space above input */
}

#message-input {
    flex-grow: 1;
    padding: 10px;
    border: 1px solid #555;
    border-radius: 4px 0 0 4px; /* Combine with button */
    background-color: #333;
    color: #eee;
    font-size: 1em;
    outline: none; /* Remove default outline */
}
#message-input:focus {
     border-color: #007bff;
}


#send-button {
    padding: 10px 15px;
    border: none;
    background-color: #007bff;
    color: white;
    cursor: pointer;
    border-radius: 0 4px 4px 0; /* Combine with input */
    font-size: 1em;
    transition: background-color 0.2s ease;
}

#send-button:hover {
    background-color: #0056b3;
}

#send-button:disabled {
    background-color: #555;
    cursor: not-allowed;
}
#message-input:disabled {
     background-color: #444;
}

.loading, .error-message {
    text-align: center;
    padding: 8px;
    margin-top: 10px;
    font-style: italic;
    border-radius: 4px;
}

.loading {
    color: #aaa;
}

.error-message {
    color: #ff6b6b; /* Red for errors */
    background-color: #4d2020;
    border: 1px solid #7a3b3b;
    font-style: normal;
    font-weight: bold;
}

footer {
    text-align: center;
    margin-top: 20px;
    font-size: 0.85em;
    color: #777;
    border-top: 1px solid #444;
    padding-top: 15px;
}
</code>

